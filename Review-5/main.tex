           % ################################### %
           % DO NOT CHANGE THIS PART OF THE FILE%
           % ################################### %

\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
           % ################################### %
           % ################################### %
           % ################################### %



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-5}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{A Learning Algorithm for Boltzmann Machines}
\end{center}
           % ################################### %


Increasing interest in parallel search and connection-based methods has motivated efficient learning algorithms. Learners aim to represent the intricate aspects of data by communicating the compatibility of entities within themselves. One such learner is the Boltzmann Machine which is trained using an energy-based objective. The work presents a novel algorithm for training Boltzmann Machines which is based on energy compatibilities between the input and output representations. Learning is carried out in an architecture consisting of visible and hidden computational units utilizing a Boltzmann distribution to predict the probabilities of spatial representations. These probabilities are collected towars the end of each trial and used in the update rule for parameters of the network. The network is tested on a number of bit encoding tasks. 

The Boltzmann Machine is a generative model which produces a representation of the input. Training of Boltzmann Machines is greatly hindered by computational speed and the choice of a suitable update rule. While the binar threshold rule serves as a potential candidate, it does not allow the network to be generalized to different inputs and tasks. An alternate method for training Boltzmann Machines is presented which consists of an update rule based on probabilistic inference. The update rule is obtained from relative entropy (KL-divergence) between the distributions of visible units when the network is trained and when the network is running freely. The information theoretic metric yields stable first order updates and reduced local convergence in comparison to the steepest descent method. The Boltzmann Machine, when trained and tested with the novel information theoretic metric on various binary code encoding tasks, demonstrates suitable performance and improving success rates. In the case of the 40-10-40 encoder, the network achieves 98.6\% success rate within 1200 cycles. 


The Boltzmann Machine utilizes an energy-based objective which depicts three key benefits in comparison to steepest descent methods. Firstly, the method can approximate first and second order derivatives accurately. Secondly, the metric is an apt update based on the similarity between the two distributions for generalization. 
\end{document}
