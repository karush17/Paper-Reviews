
\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-6}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Dependence Measures Bounding the Exploration
  Bias for General Measurements}
\end{center}
           % ################################### %

Generalization bounds have played a significant role in strengthening learning algorithms. However, their development in settings related to control measurements and adaptive data analysis remain scant. To that end, the work proposes a novel generalization bound based on $\phi$-divergences which generalizes to long-tail distributions which arise in most practical learning scenarios such as controlling measurements and reinforcement learning. 

Existing generalization bounds in literature are based on the sub-Gaussianity assumption and do not generalize well to long-tailed distributions. Moreover, these bounds are based on a number of specific rules which hinder their application to practical learning settings. The novel bound proposed in the work generalizes to all distributions with non-trivial moment generating functions. The bound introduces new dependent measure $I_{\alpha}(X;Y)$ which retains the key properties of mutual information. The bound makes use of $\phi$-divergences as these are the only decomposable divergences that satisfy the data processing inequality. The bound is generalizable using Holder's inequality and Fenchel-Young inequality by following the same framework. Tightness of the bound is theoretically evaluated using extreme value theory and the exploration bias with $\beta$-norm bounded is upper bounded by $n^{\frac{1}{\beta}}$ with $2 \leq\beta<\infty$. Additionally, results obtained in the work are found to be soft generalizations of hard results pre-existing in literature. 

The work makes a clever use of $\phi$-divergences for devising bounds specific to adaptive data analysis and controlling bias in measurements. The bound is theoretically tighter and holds in scenarios wherein previous bounds fail to demonstrate suitable results. However, the bound poses two shortcomings. Firstly, $\phi$-divergences are defined as power functions over norms. However, the method depends on the usage of affine functions to define $\phi$-divergences which is an altered definition and may not hold in the more generalized case. Secondly, the work draws close connections to generalizations of mutual information such as Sibson's mutual information and joint mutual information. However, the connection lacks a mathematical and empirical foundation demonstrating how the novel bound may be used as a substitute to pre-existing methods.



\end{document}
