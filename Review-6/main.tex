
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-6}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Dependence Measures Bounding the Exploration
  Bias for General Measurements}
\end{center}
           % ################################### %

Generalization bounds have played a significant role in strengthening learning algorithms. However, their development in settings related to control measurements and adaptive data analysis remain scant. To that end, the work proposes a novel generalization bound based on $\phi$-divergences which generalizes to long-tail distributions which arise in most practical learning scenarios such as controlling measurements and reinforcement learning. 

Existing generalization bounds in literature are based on the sub-Gaussianity assumption and do not generalize well to long-tailed distributions. Moreover, these bounds are based on a number of specific rules which hinder their application to practical learning settings. The novel bound proposed in the work generalizes to all distributions with non-trivial moment generating functions. The bound introduces new dependent measure $I_{\alpha}(X;Y)$ which retains the key properties of mutual information. The bound makes use of $\phi$-divergences as these are the only decomposable divergences that satisfy the data processing inequality. The bound is generalizable using Holder's inequality and Fenchel-Young inequality by following the same framework. Tightness of the bound is theoretically evaluated using extreme value theory and the exploration bias with $\beta$-norm bounded is upper bounded by $n^{\frac{1}{\beta}}$ with $2 \leq\beta<\infty$. Additionally, results obtained in the work are found to be soft generalizations of hard results pre-existing in literature. 

The work makes a clever use of $\phi$-divergences for devising bounds specific to adaptive data analysis and controlling bias in measurements. The bound is theoretically tighter and holds in scenarios wherein previous bounds fail to demonstrate suitable results. However, the bound poses two shortcomings. Firstly, $\phi$-divergences are defined as power functions over norms. However, the method depends on the usage of affine functions to define $\phi$-divergences which is an altered definition and may not hold in the more generalized case. Secondly, the work draws close connections to generalizations of mutual information such as Sibson's mutual information and joint mutual information. However, the connection lacks a mathematical and empirical foundation demonstrating how the novel bound may be used as a substitute to pre-existing methods.

The work opens two new avenues for future work. Firstly, the bound demonstrates the suitability of $\phi$-divergences due to their unique property of satisfying the data processing inequality. The natural question arises whether other general metrics can be adopted to the bound and improve its tightness. Secondly, soft generalizations of hard results presented in the work are not a direct generalization and depend on additional analysis. One may ask that what other soft generalizations are realizable and how can one achieve them?

Growing advances in generalization theory have motivated dependent bounds which constrain the generalization error. However, these bounds are seldom applicable to control measurements. The work proposes a novel bound on exploration bias with regard to control settings such as reinforcement learning. The bound proposed makes use of $\phi$-divergences in order to generalize to long-tail distributions with non-trivial moment-generating functions. The bound makes use of dependent measures which replace and retain the key propoerties of mutual information. Tightness of the bound is theoretically validated using extreme value theory with soft generalizations of results obtained from hard expressions. The bound developed motivates the usage of dependent measures and divergence metrics to practical settings.

\end{document}
