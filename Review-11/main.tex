
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-11}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{LEAF: Latent Exploration Along the Frontier}
\end{center}
           % ################################### %
Real-world robot tasks require the use of efficient exploration strategies. These strategies are facilitated by goal proposal and temporally abstract schemes. However, these do not necessarily aid in discovering long horizon plans. To that end, the work proposes an exploration framework consisting of committed exploration along a localized set of states. Latent Exploration along the Frontier (LEAF) allows the robot to explore states near a frontier in latent space which has been previously explored. The LEAF policy consists of a deterministic component which places the robot at the frontier of states to be explored. The stochastic component, on the other hand, allows the agent to explore novel states. Simultaneous combinations of stochastic and deterministic executions result in committed exploration and efficient allocation of novel state budget. LEAF demonstrates improved exploration performance on challenging environments including a real robot task. 

LEAF carries out exploration in latent space with pixel and state-based inputs. Corresponding to a frontier, the robot receives goal and input states. Reachability of the goal state from the initial state is evaluated using a $\beta-VAE$ which yields encodes both states and infers the frontier from the learned manifold. The $VAE$ yields a binary valued output corresponding to whether the goal is reachable or not. In the case of reachable goals, the robot executes its deterministic policy to reach a state in the frontier and execute its stochastic policy to reach the final goal state. In order to facilitate reachability towards goals, the robot is made to sample easy-to-reach goal states towards the beginning of training using an implicit curriculum. Training additionally consists of $\beta-VAE$ which is optimized to minimize the distance between the initial and goal states. 



\end{document}
