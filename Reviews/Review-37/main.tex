
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-37}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement}
\end{center}
           % ################################### %

Successor Features (SFs) in conjunction with Generalized Policy Improvement (GPI) have enabled successful transfer of skills in Reinforcement Learning (RL). However, the SF\&GPI framework assumes rewards as linear combinations of fixed set of features. The work presented in the paper rerlaxes this constraint and generalizes SF\&GPI to any set of tasks that only differ in the reward function. One can use reward functions themselves as features for future tasks, thus reducing the need to specify features beforehand. The proposed scheme learns online and demonstrates instantaneous transfer performance on challenging 3D environments in comparison to baseline methods. 

The work generalizes the framework of SF\&GPI by inducing a set of MDPs using successor features which are linearly independent of each other. This allows rewards to be expressed as linear combinations of successor features and weight vectors serving as approximations. The framework further presents that rewards can themselves be used as features since successor feature vectors are a linear representation of rewards. Such a formulation allows for a more convenient approximation which can be easily learned in the more general case. The generalized SF\&GPI method is implemented using $\epsilon$-greedy $Q$-learning by treating $Q$-values as inner products of successor feature sets and weight vectors. Incorporating rewards as features allows for simpler approximations in this algorithm by directly updating the policy with respect to reward approximations. 

Proposed SF\&GPI framework demonstrates improved, in some cases instantaneous, transfer performance on test tasks when executed on challenging 3D environments. Another notable finding is that the agent is able to perform well on tasks with negative rewards even though it was trained only on tasks with positive rewards. This indicates that the transfer policy is successfully combining policies corresponding to base tasks. The work further compares the generalized framework with prior method of linearly-dependent base tasks. The generalized framework, having an upper bound on $Q$-value errors, presents sub-optimal performance in comparison to linear base tasks. Furthermore, the scheme extends utilization of rewards as features based on significant assumptions which may not hold true in practical scnearios. For instance, transferring of features assumes that base tasks are linearly independent of each other which may not be true in the case of temporally-extended environments. Lastly, scheme takes into account various sources of error but does not throw light on approximation bias of $Q$-values which comprise of feature and weight vector estimates. The question of whether this induces inherent biases in estimates remains as open problem. 

The generalized framework of SF\&GPI demonstrates improved transfer performance on test tasks based on linearly-independent base tasks. The scheme can be further extended towards nonlinear settings which otherwise require a more rigorous theortical treatment for policy generalization. Another interesting avenue of work would be to address some of the limitations established on the linear case which may not hold true for real-world navigation tasks. 

SF and GPI coalesce readily to provide a suitable framework for transferring policies from base to test tasks in RL. The paper presents an extension of the SF\&GPI framework by generalizing it to linearly independent base tasks. Through this lens, the rewards can be utilized as features for future tasks, hence eliminating the need for obtaining features beforehand. The generalized scheme can be looked at as another instance of TD learning with rewards as features, target values as future feature sets and estimates as current feature sets. SF\&GPI presents improved transfer performance on test tasks with an interesting direction for future work. 

\end{document}
