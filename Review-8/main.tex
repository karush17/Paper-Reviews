
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-8}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Hindsight Credit Assignment}
\end{center}
           % ################################### %

Assigning credit to past decisions has been a challenging problem due to high variance and bootstrapped estimates. The work addresses this open problem by assigning credit to past decisions in hindsight. More specifically, the work aims to answer the question \textit{"given a state $x$, how does choosing an action $a$ affect the returns?"}. Taking into account the four main hindrances for efficient credit assignment, the work proposes a novel Hindsight Credit Assignment (HCA) scheme based on conditioned future states (HCA$|$State) and future returns (HCA$|$Return). The proposed HCA scheme is found to be efficient in assigning credit to past actions in comparison to policy gradient.

The significance of an action in a given trajectory is challenging to estimate. High variance in value estimates yields randomness in trajectories. In the case of partial observability, temporal difference learning is hindered by bootstrapped estimates which lead to inaccurate approximations. Additionally, a single trajectory may not incorporate information about all actions. To that end, the novel HCA scheme consists of future conditionals which weigh the importance of an action with respect to agent's policy. (HCA$|$State) consists of a state-conditional hindsight distribution which quantifies the relevance of a past action to a future state. Similarly, (HCA$|$Return) comprises of the reward-conditional hindsight distribution which conditions actions on future rewards. Hindsight distributions in both schemes can be generalized to time-independent distributions which yield the probability of taking an action in a some future state.   

\end{document}
