
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-12}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings}
\end{center}
           % ################################### %

Real-world settings require the Reinforcement Learning (RL) agent to cautious yet optimal behavior. To that end, the work presents a Safety Critical Adaptation (SCA) framework which allows the agent to adapt to scenarios by executing a safe policy. The framework trains the agent in a non-safety critical simulation setting in order to adapt to safety-critical scenarios where failures penalize the agent heavily. The work further proposes a solution based on SCA framework in order to yield risk-averse policies for cautious adaptation. Cautious Adaptation in Reinforcement Learning (CARL) makes use of model-based RL to capture uncertainity and estimate risk followed by planning in sfety-critical settings to avoid catastrophic states. CARL empirically demonstrates risk-averse behavior with fewer failures in comparison to RL baselines. 

In most real-world scenarios, agents require safe behavior which would enable risk-averse policies in unforeseen tasks. Such a kind of knowledge transfer results in safe and data efficient adaptation of the agent in the new environment. SCA framework allows safe adaptation in RL by pretraining the agent in a setting where safety is not a concern. Following pretraining, the agent is allowed to adapt to safety-critical settings where catastrophic behavior is heavily penalized. In order to dmeonstrate the suitability of safe adaptation using SCA, the work proposes CARL, which is based on the intuition that agents can estimate risk and reason about uncertainity while learning from a variety of scenarios. CARL is pretrained using an ensemble of probabilistic models on various sandbox environments where safety is not a concern. The framework is combined with PETS but the method is generalizable to any model-based approach. Following pretraining, CARL is made to adapt to safety-critical scenarios where it plans to select actions leading to risk-averse states. Planing is carried out using a novel reward computation scheme which assigns an action score to each planned trajectory. CARL minimizes risk using two variants. Firstly, the agent may seek high rewarding states in order to minimize the risk of low rewards. This is governed using a caution parameter. Secondly, the agent may minimize the risk of reaching catastrophic states. State risk-aversion is based on minimizing the proability of critical states serving as reward penalty. 





\end{document}
