
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-18}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Weighted QMIX: Expanding Monotonic Value
  Function Factorisation for Deep Multi-Agent
  Reinforcement Learning}
\end{center}
           % ################################### %

Suitability of QMIX in Multi-Agent Reinforcement Learning (MARL) has been pivotal in training decentralized policies in the centralized setup. While QMIX aids in an expressive joint $Q$-function utilizing monotonic constraints in mixing, it often fails to recover representations wherein the ordering of actions may depend on other agent's actions. This requires one to rethink the mapping between $Q$-space and $Q_{mix}$-space from an operational point of view. The work throws light on this aspect by formulating the QMIX operator which demonstrates nuances in its projections. In order to address these nuances, two novel weighting schemes are proposed which place more importance on better joint actions. Centrally-Weighted QMIX (CW-QMIX) and Optimistically-Weighted QMIX (OW-QMIX) demonstrate improved performance on StarCraft II micromanagement scenarios. 

The incorrect $argmax$ estimations of QMIX projections demonstrate its limitation to recover optimal policies in case of non-monotonic scenarios. The problem is further highlighted by observing that QMIX restricts function class approximations which fail to suitably project $Q$-values in the set of all factorizations, hence resulting in a gap (action gap) between $Q_{tot}$ and $Q^{*}$. This is evident from the formulation of QMIX as an operator which does not possess a unique fixed point. Additionally, the operator underestimates the value of optimal joint action which is a direct consequence of inaccurate $argmax$ approximations in projection space. To this end, the work introduces weighting in the original QMIX objective which places importance on favourable joint actions. The initial QMIX objective comprises of a unit weighting. On the other hand, proposed weighting schemes downweight $Q$-values which either represent underestimations or sub-optimal actions. While CW-QMIX strictly weights the importance of joint action w.r.t optimal approximates $\hat{Q}^{*}$, OW-QMIX optimistically weights $Q_{tot}$ exactly. 

CW-QMIX and OW-QMIX demonstrate improved performance on SMAC StarCraft II micromanagement scenarios and Predator Prey which require greater collaboration per timestep. In addition to performance, the weighting schemes depict robustness to increased exploration indicating suitability of weighting in retrieving expressive joint action representations. While the weighting scheme stabilizes the performance and expressivity of mixing, it presents two shortcomings. Firstly, in case of scenarios with large number of agents and difficuly dynamics, CW-QMIX and OW-QMIX require extensive exploration to converge to an optimal policy. This may indicate that weighting Bellman updates leads to sample-efficient learning. Lastly, the work highlights the limitation of learning $\hat{Q}^{*}$ as a potential limitation since it aggregates complexity in the network architecture. This may be addressed using a more sophisticated robust weighting scheme which does not affect $Q$ with the increasing number of agents. 




\end{document}
