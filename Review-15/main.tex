
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-15}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Evaluating Agents Without Rewards}
\end{center}
           % ################################### %

Reinforcement Learning (RL) makes use of complex reward functions which need to be hand-designed and engineered. Reward formulations are prone to human error and susceptible to sparsity. To this end, the work revisits objectives which enable the agent to learn without task rewards. Estimation and application of these objectives remains an open question and the work aims to throw light in this aspect. The study explores correlation between objectives and human behavior across seven agents and three Atari games.

A total of three RL objectives are considered which do not rely on task rewards. Firstly, curiosity encourages the agent to seek rare inputs using a learned density model. Empowerment, on the other hand, measures the agent's influence over its environment. Lastly, Information Gain rewards the agent while discovering rules in the environment. The study compares objectives on Breakout, Seaquest and Montezuma's Revenge from the Atari benchmark by making use of PPO, ICM and RND agents. Metrics are additionally compared to pre-recorded human behavior, no-op and random agents. In order to preprocess pixel inputs, a simple preprocessing scheme consisting of resizing and discretization is adopted to preserve the position of objects based on brightness percentiles. Images are then enumerated to represent each unique frame by an integer index. Indexing is carried out to evaluate discrete probabilities which are utilized in the agent's comparison with human behavior. Comparison between agents is carried out using correlation metrics which assess the degree of similarity between objectives and task rewards, and objectives and human behavior. 

Out of the three objectives considered, curiosity correlates strongly with human behavior. Task rewards with curiosity better explain task rewards alone. Empowerment and Information Gain correlate strongly with each other but demonstrate a weaker relationship to curiosity. While the study aims to understand task-agnostic behavior in RL, it lacks several essential components in its analysis. Firstly, the work only exploits probabilities based on state visitations during agent's lifetime which may not be a sufficient component. One could also employ attention mechanisms and kernel visualizations to monitor which pixels the agent finds important. In comparison to human behavior, curiosity is found to be beneficial in presence of task rewards which highlights the necessity of rewards. The study does not provide insights into wide variations in results of empowerment and information gain and argues that these may vary as per the nature of the game. To this end, task-agnostic objectives may not necessarily scale to high-dimensional action spaces. Lastly, the simple preprocessing scheme and limited human data hinder the reliability of analysis. 

While the study presents several limitations in its experimental setup and analysis, it presents two novel directions for future work. The comparison of RL agents with human behavior is a novel direction which may be adopted for future research. One can compare other approaches such as model-based RL with human behavior. Lastly, technical components of the study such as preprocessing and evaluation metrics can be extended to vision-based methods which would allow one to gain insights towards agent's behavior. 

Engineering reward functions specific to a task is prone to human error and hinders learning in case of sparse metrics. The work turns its attetion to reward-agnostic RL objectives and compares their similarity to human behavior. A total of seven agents are compared across three different Atari games which make use of Curiosity, Empowerment and Information Gain. Agents are assessed based on their correlation with human behavior for their overall training time. Curiosity depicts strong similarity with human behavior and promise for future research in reward-agnostic RL. 
\end{document}
