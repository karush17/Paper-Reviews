           % ################################### %
           % DO NOT CHANGE THIS PART OF THE FILE%
           % ################################### %

\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
           % ################################### %
           % ################################### %
           % ################################### %



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-3}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{ When to use parametric models in reinforcement learning?}
\end{center}
           % ################################### %

Growing advances in model-based reinforcement learning have yielded data-efficient methods. These methods learn a model of the dynamics of the world and align behaviors of the agent with the model's beliefs. However, it is often unclear as to when to use a model for acting and planning since models may inherently be imperfect in nature and can steer the agent towards a sub-optimal policy in the case of inaccurate beliefs. The work investigates the usage of parameterized models and their relationship to replay memory in various reinforcement learning settings. Replay memory is thought of as analogous to a model being used for behavioral updates. Such behavioral models are used for training agents in a sample-efficient manner in comparison to pure model-based approaches.



\end{document}
