
\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-14}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Skill Transfer Via Partially Amortized Hierarchical Planning}
\end{center}
           % ################################### %

Solving new tasks in complex environments requires effective transfer of knowledge. To this end, the work proposes a novel algorithm for transfering skills in a partially amortized framework by adopting hierarchical planning for selection of skills. A Reinforcement Learning (RL) agent plans to compose skills in imagination which are used to condition the low-level behavioral policy for learning and optimally acting in the environment. Planning is carried out be making use of a world model which allows sample-efficient learning. Planning in low-dimensional skill-space allows the agent to suitably transfer knowledge across different tasks. 

Hierarchical planning in a partially amortized setting combines the fully online and fully amortized methods for acting. Online planning is carried for selecting skills which are used to condition on the low-level policy. Agent optimizes its low-level policy in the fully amortized setting. Learning of the agent is carried out by making use of a world model similar to Dreamer. Model learning consists of training the representation, observation, latent-state and task-reward modules. High-level skills are held fixed for $K$ number of steps and optimized using CEM. Behavioral learning consists of training the lowe-level policy in world model using the skills distribution and backward skill predictor. Lastly, the policy interacts in the environment using MPC with a pre-sampled high-level skill. In order to facilitate learning of skills, the method constructs an auxilary objective which maximizes the Mutual Information (MI) between latent skills and state sequences. This requires a tractable distribution which is adopted using the backward skill predictor and trained using supervised learning in conjunction with reparameterization of CEM distribution. 



\end{document}
