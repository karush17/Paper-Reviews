           % ################################### %
           % DO NOT CHANGE THIS PART OF THE FILE%
           % ################################### %

\documentclass[10pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{lastpage}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\hypersetup{%
  colorlinks=true,
  linkcolor=blue,
  linkbordercolor={0 0 1}
}
\renewcommand\lstlistingname{Algorithm}
\renewcommand\lstlistlistingname{Algorithms}
\def\lstlistingautorefname{Alg.}

\lstdefinestyle{Python}{
    language        = Python,
    frame           = lines, 
    basicstyle      = \footnotesize,
    keywordstyle    = \color{blue},
    stringstyle     = \color{green},
    commentstyle    = \color{red}\ttfamily
}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
           % ################################### %
           % ################################### %
           % ################################### %



\pagestyle{fancyplain}
\headheight 35pt
                 % ENTER REVIEW NUMBER HERE %
\chead{\textbf{\large Review-1}}
           % ################################### %

\lfoot{}
\cfoot{}
\rfoot{\small\thepage}
\headsep 1.5em

\begin{document}


                 % ENTER PAPER TITLE HERE %
\begin{center}
  \large{Action and Perception as Divergence Minimization}
\end{center}
           % ################################### %


Divergence minimization has seen tremendous growth in designing novel Reinforcment Learning (RL) objectives over the past few years. A good choice of RL objective is essential in maintaining a balance between informative exploration and inferring environment representations. To that end, the work presents a unified objective which combines the key elements of action and perception as joint divergence minimization. The key idea behind the objective is to extract relevant information from representations observed by the agent and utilize them in an efficient manner to align future dynamics as per the beliefs of the agent.

Various information-based objectives in literature such as inference methods, information gain, entropy-based exploration and free-energy minimization principle can be thought of as a joint divergence minimization approach. The unified objective consists of an actual distribution (which is a generative process parameterized by the agent) and a target distribution (which is desired by the agent). The joint divergence minimization makes use of the Kullback-Liebler (KL) divergence metric to bring the actual distribution closer to the target distribution by optimizing over the agent's parameters. The optimization of the agent consists of two stages. (1) Firstly, the perception phase aligns beliefs of the agent with its past inputs based on inference of latent variables. These latent variables are not observed over time and can only be utilized in an indirect manner such latent representations in representation learning or model parameters in the case of model learning. (2) Secondly, the action phase aligns the future inputs with the agent's beliefs by making use of latent variables estimated in the perception phase. These typically are made use of in a future term such as entropy in exploration, skill discovery in hierarchical RL or information gain in model learning. The combinationg of action and perception renders task rewards optional since the agent itself learns in latent space.

The work presents a detailed review of various information-based objectives and their retrieval from the unified action-perception objective. For instance, the divergence minimization is a sound generalization of inference methods such as variational and amortized inference; exploration techniques such as maximum entropy RL and empowerment; and temporal abstraction such as skill discovery. Moreover, the review maintains a close connection between the actual and target distributions highlighted in the definition of the objective which provide a generalized procedure for constructing novel objectives for future work. This can be achieved by treating the target distribution as a probabilstic model under which the agent infers informative representations and explores future inputs. On the other hand, the unified objective itself is constructed of primitive entities observed in literature and does not give rise to a new objective. For instance, the objective is a theoretical collection of past objectives which are linked using the divergence minimization scheme. Moreover, the framework proposed is dense and throws little light on how to practically use the unified objective in the formulation of novel information-based methods. Lastly, the unified objective mentions connections to a few essential methods such as surprise minimization but does not take them into account as a significant constituent.

The work opens two new directions for future work. Firstly, theoretical unification of information schemes paves the way for practical applicability for constructing new objectives. Secondly, the divergence minimization itself can be broadened by adding novel elements to the actual distribution based on the past and future of the agent. For instance, with the reward being an optional entity in the agent's observation space, can it be effectively leveraged in latent space? If not, then what other entities of the agent's dynamics could be used for effective latent exploration? While the incorporation of new latent variables remains unclear, the unified objective succeeds in steering research towards this direction.

The work presents a unified objective for divergence minimizaiton consisting of action and perception as stages. The perception stage aims to align the latent beliefs of the agent with its input, while the action stage soughts to align future inputs with the latent beliefs. The agent makes use of the two stages in order to approximate a distribution, over its latent variables, with respect to a desired target distribution. The unified objective serves as a generalization for retrieving various RL objectives with the requirement of task rewards rendered as an optional metric. The unified objective serves as a general recipe for devising novel information-based objectives but does not yield an application for the same. However, the work paves the way for utilizing agent dynamics in latent space and its practical application to real-world RL applications.   


\end{document}
